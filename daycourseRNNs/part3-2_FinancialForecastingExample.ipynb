{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN Time Series Forecasting (EUR-USD Financial Dataset example)\n",
    "\n",
    "\n",
    "## IADS Summer School, 3rd August 2021\n",
    "\n",
    "### Dr Michael Fairbank, University of Essex, UK\n",
    "\n",
    "- Email: m.fairbank@essex.ac.uk\n",
    "- This is Jupyter Notebook 3.2 of the course\n",
    "\n",
    "## Objective\n",
    "\n",
    "- In this notebook we will use a RNN to forecast the EUR/USD exchange rate.\n",
    "\n",
    "- Note that apart from the dataset and graph plotting, the code in this demo is almost interchangable with the code from the previous demo.  \n",
    "\n",
    "- We are just trying the same method to forecast a different set of numbers!  (Replacing \"Temperature\" by \"Prices\".)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load dataset\n",
    "\n",
    "We use Pandas to load the csv file very easily in Python.  Pandas also lets us easily manipulate and view the data.\n",
    "\n",
    "This dataset came from my own Interactive Brokers account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "df = pd.read_csv(\"datasets/eurusd60s_2013_2018.csv.zip\", compression='zip', sep=',') # This data is minute-by-minute data.\n",
    "# If the above line fails then try the following line instead, to download it directly from \n",
    "# my google drive account\n",
    "#df = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1o052vfAJRCKcZD2-fLLe-9CBQd-WWhjN\", compression='zip', sep=',')\n",
    "\n",
    "df = df[0::(60)] # Reduce data down to hourly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take a glance at the data. Here are the first few rows.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(df.describe().transpose())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note the volume data is missing for this Forex data.\n",
    "\n",
    "- In real trading you buy at the ask prices and sell at the bid prices, so always lose a bit of money due to the difference in any transaction.  Plus there are usually other commission costs and price slippage.  We will only look at bid prices in this exercise.\n",
    "\n",
    "- The high and low prices for each bar were calculated on the minute-by-minute data so are invalid now we have trucated down to hourly data.  Hence we should only look at open_bid price in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## View data graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = plt.gca()\n",
    "formatter = mdates.DateFormatter(\"%Y\")\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "locator = mdates.YearLocator()\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "plt.plot(pd.to_datetime(df['date']), df['bid_open']) # Only look at one of the price columns (for simplicity)\n",
    "plt.title('Times Series for EUR/USD');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculate log returns, and simplify dataset\n",
    "\n",
    "The log return of a price at time $p_t$ is $log(p_t/p_{t-1})$.  This tells you the ratio that a price has changed by in any one time step.  It will be positive if $p_t>p_{t-1}$ and negative otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "price_col=df[\"bid_open\"].values\n",
    "price_return=np.log(price_col[1:]/price_col[:-1])\n",
    "price_col=price_col[1:]\n",
    "df=pd.DataFrame(columns=[\"price\",\"return\"], data=np.stack([price_col,price_return],axis=1))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split the Data\n",
    "\n",
    "- As before, we'll use a `(70%, 30%)` split for the training and validation. \n",
    "\n",
    "- As before, the data is **not** being randomly shuffled before splitting. \n",
    "\n",
    "- We must not shuffle the validation set into the training dataset - that would be cheating ourselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Normalize the data\n",
    "\n",
    "It is important to scale features before training a neural network. Normalization is a common way of doing this scaling. Subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "The mean and standard deviation should only be computed using the training data so that the models have no access to the values in the validation (and test) sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "#test_df = (test_df - train_mean) / train_std\n",
    "print(\"Normalised input data\\n\", train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choose input columns and target output columns for forecasting objective:\n",
    "\n",
    "Here we decide which columns to show the neural network, and which columns we'd like it to be trained to predict.  \n",
    "- We are going to try to forecast future prices based on old prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_columns=[0]\n",
    "target_columns=[0] # This is the price column \n",
    "print(\"Input columns\", list(train_df.columns[input_columns]))\n",
    "print(\"Target columns\", list(train_df.columns[target_columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prepare input and output time sequences\n",
    "\n",
    "We want the RNN to receive a stream of data, corresponding to the above input columns, spread over time, and to make a forecast of the target column quantity for look_ahead timesteps ahead.\n",
    "\n",
    "We will prepare this data as 3d tensors suitable for input to Keras RNNs, so of shape [batch_size, time_sequence_length, data_width]\n",
    "\n",
    "Note we say a time_sequence_length=40, and allow a warm_up_period=30, which gives the RNN 30 timesteps to observe the context on what the time series is doing before it is being challenged to make any predictions.  It will then have to make 10 consecutive predictions, where each prediction is look_head=1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def generate_input_and_target_tensor(df, input_cols, target_col, time_sequence_length, warm_up_period, look_ahead):\n",
    "    num_rows=len(df)\n",
    "    data=np.array(df,np.float32)\n",
    "    inputs=[]\n",
    "    targets=[]\n",
    "    for first_row in range(num_rows-time_sequence_length-look_ahead):\n",
    "        inputs.append(data[first_row:first_row+time_sequence_length,input_cols])\n",
    "        targets.append(data[first_row+look_ahead+warm_up_period:first_row+look_ahead+time_sequence_length,target_col])\n",
    "    return np.stack(inputs), np.stack(targets)\n",
    "\n",
    "        \n",
    "time_sequence_length=40\n",
    "warm_up_period=30\n",
    "look_ahead=1\n",
    "[inputs_tensor_train,targets_tensor_train]=generate_input_and_target_tensor(train_df, input_columns, target_columns, time_sequence_length, warm_up_period, look_ahead)\n",
    "[inputs_tensor_val,targets_tensor_val]=generate_input_and_target_tensor(val_df, input_columns, target_columns, time_sequence_length, warm_up_period, look_ahead)\n",
    "print(\"inputs\",inputs_tensor_train.shape)\n",
    "print(\"targets\",targets_tensor_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Build RNN (LSTM) Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#build model\n",
    "inputs = keras.Input(shape=(time_sequence_length,len(input_columns)), name='input')\n",
    "#hidden_vectors3d = layers.SimpleRNN(20, return_sequences=True,activation='tanh')(inputs)\n",
    "hidden_vectors3d = layers.LSTM(20, return_sequences=True,activation='tanh')(inputs)\n",
    "if warm_up_period>0:\n",
    "    # chop off the warm-up period\n",
    "    hidden_vectors3d=hidden_vectors3d[:,warm_up_period:,:]\n",
    "# surprisingly an ordinary Dense layer can act on a 3d tensor in the way we need here....\n",
    "output_vectors3d=layers.Dense(len(target_columns))(hidden_vectors3d)\n",
    "model = keras.Model(inputs=inputs, outputs=output_vectors3d)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compile + Train Model\n",
    "\n",
    "Specify the optimiser (Adam) and the loss function to use.  This is a regression task (we are forecasting a real number) so we use MeanSquaredError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.01),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.MeanSquaredError(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[tf.metrics.MeanAbsoluteError()],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    inputs_tensor_train,\n",
    "    targets_tensor_train,\n",
    "    batch_size=100,\n",
    "    epochs=10,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(inputs_tensor_val, targets_tensor_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualise training progress\n",
    "- We can use these graphs to decide how much \"overfitting\" might have happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"],label=\"Val loss\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# View some individual forecasts\n",
    "Hopefully we can see that the price forecasts are reasonable. \n",
    "\n",
    "Again, we reverse the standard-deviation normalisation used in the data-preparation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_std=np.array(train_std[target_columns])\n",
    "train_mean=np.array(train_mean[target_columns])\n",
    "for row in range(0,40,4):\n",
    "    print(row,\"targets\",(targets_tensor_val[row,:,0]*train_std+train_mean))\n",
    "    print(row,\"prediction\",(model(inputs_tensor_val[row:row+1]).numpy()[0,:,0]*train_std+train_mean))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plot forecast graph\n",
    "\n",
    "Let's try to show graphically the first 40 points forecast in the validation set, and compare to their targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "targets_=[]\n",
    "predictions_=[]\n",
    "for row in range(40):\n",
    "    targets_.append(targets_tensor_val[row,-1,0]*train_std+train_mean) # here we are truncating the time series so tha only the final timestep is selected\n",
    "    predictions_.append(model(inputs_tensor_val[row:row+1]).numpy()[0,-1,0]*train_std+train_mean)\n",
    "plt.ylabel(train_df.columns[target_columns][0])\n",
    "plt.plot(range(len(targets_)), targets_,label='Labels', marker='.', zorder=-10)\n",
    "plt.scatter(range(len(predictions_)), predictions_,edgecolors='k', label='Predictions', c='#2ca02c', s=64)\n",
    "plt.legend()\n",
    "plt.xlabel('Time step [h] (forecast length='+str(look_ahead)+'h)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting result\n",
    "\n",
    "This result looks reasonably good, but can you use it to actually make money?  \n",
    "\n",
    "**Question:** If the price of AMZN stock today is 2000 dollars and 24 months ago it was 800 dollars, what magnitude would you forecast tomorrow's price to be?  2000 or 800?  Which is your personal best guess out of those two options?\n",
    "\n",
    "What do you think the forecaster is actually doing to make its forecast?  **Can you see the problem?**\n",
    "\n",
    "We can make this task much harder, and potentially much more useful, if we can change it to forecast the *change* of the price.  **Try this** by changing the target column to the \"return\" column, and retraining your network.  Then run the code on the prediction accuracy block below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction Accuracy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if target_columns==[1]:\n",
    "    # we are forecasting the \"return\" column, which should be positive every time the price goes up, \n",
    "    # and negative whenever it goes down.  Let's do a sign count to see how good it is.\n",
    "    # Do a sign count:\n",
    "    predictions=model(inputs_tensor_val).numpy()[:,-1,0]*train_std+train_mean # calculate the final RNN prediction only (since this should be the best one having had the longest warm-up period)\n",
    "    targets=targets_tensor_val[:,-1,0]*train_std+train_mean\n",
    "    baseline_model=np.mean(targets) # This model just predicts the same each time - it's useful as a baseline for comparison.\n",
    "    correct_predictions=(predictions*targets>0).astype(np.float32) # This will give an array of 1s and 0s showing whether each individual prediction has the correct sign or not.\n",
    "    correct_baseline_predictions=(baseline_model*targets>0).astype(np.float32) # Same as above, but for baseline predictor\n",
    "    print(\"predictions\",predictions)\n",
    "    print(\"targets\",targets)\n",
    "    print(\"correct_predictions\",correct_predictions)\n",
    "    print(\"accuracy on validation set\",np.mean(correct_predictions))\n",
    "    print(\"baserate accuracy on validation set\",np.mean(correct_baseline_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can you improve performance at all?\n",
    "\n",
    "- If we want the neural network to spot commonly repeating patterns in the changes of price, then we should also change the *inputs* also to be returns, not prices.  **Try this.**\n",
    "\n",
    "- Also you can change the number of time steps the RNN gets to look at before it has to make a forecast.\n",
    "\n",
    "- Try changing the forecast lookahead period.  \n",
    "    - In this case be careful to also redefine the way returns are defined to be based upon this new look-ahead distance.\n",
    "\n",
    "Do you have any other ideas to try yourself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further work\n",
    "\n",
    "Try to add fancier layers (e.g. dropout layers)\n",
    "\n",
    "Add more useful inputs, e.g. other correlating stocks, sentiment information, technical \"indicators\" such as moving average, RSI indicator, CCI indicator, etc.\n",
    "\n",
    "In Future work, get your own data, e.g. see https://towardsdatascience.com/time-series-and-correlations-with-stock-market-data-using-python-e66774e3a16f.  This includes sentiment data.  Also you can trawl through a wider range of commodities.\n",
    "\n",
    "Try to pre-filter the data to remove some noise, e.g. remove unintersting moments/smooth time series and add \n",
    "stochastic noise.\n",
    "\n",
    "Train ensemble of models.  The average of several independent models is nearly always better than any of the individual models (!).\n",
    "\n",
    "\n",
    "Note that we are always fighting against the fundamental problem of market's ability to adaptation and be efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common errors to avoid:\n",
    "\n",
    "1. Predict changes, not absolute price.  Absolute price can be misleading.\n",
    "\n",
    "2. Be careful about quoting results on out-of-sample data (e.g. a test set).\n",
    "    - Avoid using a test set more than once to avoid survivorship bias.\n",
    "\n",
    "3. Avoid trying to predict a stock which always moves upwards, e.g. tech stocks since 2015.\n",
    "\n",
    "4. You can't just use the accuracy (win-rate) to rate a system. \n",
    "    - We show below how to get 90% win rates without having a good system.\n",
    "    - See Andreas Krause and Michael Fairbank. \"Baseline win rates for neural-network based trading algorithms.\" International Joint Conference on Neural Networks, Glasgow, (2020).\n",
    "\n",
    "5. Publishing your method can make it not work:\n",
    "\n",
    "    - Cite McLean, R. David, and Jeffrey Pontiff. \"Does academic research destroy stock return predictability?.\" The Journal of Finance 71.1 (2016): 5-32."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
