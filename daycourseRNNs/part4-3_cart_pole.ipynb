{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cart-Pole Neurocontrol Example\n",
    "## IADS Summer School, 3rd August 2021\n",
    "\n",
    "### Dr Michael Fairbank, University of Essex, UK\n",
    "\n",
    "- Email: m.fairbank@essex.ac.uk\n",
    "- This is Jupyter Notebook 4.3 of the course\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- This notebook is an optional extension, just to show you how we can solve the pole-balancing problem using BPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The cart-pole problem\n",
    "\n",
    "![cart-pole.png](images/cart-pole.png)\n",
    "- The objective is to control the cart to move forwards and backwards so as to carefully balance the swinging pole in an upright postion\n",
    "    - The cart can be controlled by applying a linear force $F$\n",
    "    - The pole swings freely, balancing on its pivoted base.\n",
    "    - The game is over when the pole balances beyond $\\pm 12$&deg;, or the cart position $x$ exceeds $\\pm 2$, or the time limit is reached.\n",
    "\n",
    "- To see a physical cart-pole implementation, see https://www.youtube.com/watch?v=5Q14EjnOJZc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When we run this notebook, we should see the solution converge to a balanced pole.  You should see something like this:\n",
    "\n",
    "![cartpole_converged_result.png](images/cartpole_converged_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- These plots show 4 trajectories, one shown in each colour.\n",
    "    - so the pole is starting from 4 different positions, and in each case completing a full balancing simulation.\n",
    "- Look carefully at the theta graph, and you can deduce the pole wobbling about over time.\n",
    "- Look carefully at the x graph and you can see the cart's x coordidinate slowly becoming more negative (i.e. the cart is drifting left here, in each trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Control-Network with recurrence:\n",
    "- As before, we have a known physics model of the world, and treat it like a hand-built layer to build a network with a recurrent loop:\n",
    "\n",
    "<img src=\"./images/control_rnn_mike.svg\"  width=\"400\">\n",
    "\n",
    "\n",
    "- Plus we use a neural network, the \"action network\", to make the decisions which control the agent (the cart in this example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reward function\n",
    "\n",
    "The reward used to train the cart-pole is given by \n",
    "$$R=-\\gamma^T$$\n",
    "where $T$ is the the number of time steps successfully balanced for, and $\\gamma=0.97$ is the \"discount factor\".\n",
    "\n",
    "Because of the minus sign and $\\gamma<1$, the reward increases with $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The need for clipping\n",
    "Because we need a smooth reward function to perform gradient ascent on, we have to convert $T$ into a continuous real-valued number, by doing \"clipping\" on the final time step. For details, see\n",
    "\n",
    "- Fairbank, Michael, Danil Prokhorov, and Eduardo Alonso. \"Clipping in neurocontrol by adaptive dynamic programming.\" IEEE transactions on neural networks and learning systems 25.10 (2014): 1909-1920,\n",
    "\n",
    "or the function run_physics_model_one_step in the program code below.  \n",
    "\n",
    "The following graphs show how clipping makes smooth gradient ascent possible for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "duration = np.arange(0, 30, 0.1);\n",
    "fig=plt.figure(figsize=[12.4, 3])\n",
    "ax1=fig.add_subplot(1,2, 1)\n",
    "ax2=fig.add_subplot(1,2, 2)\n",
    "ax1.plot(duration, -0.9**np.floor(duration), c='r', label=\"$R=-\\gamma^{floor(T)}$\")\n",
    "ax1.set_title('Without Clipping')\n",
    "ax1.set_xlabel('Balance Duration (T)')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "ax2.plot(duration, -0.9**duration, c='b', label=\"$R=-\\gamma^{T}$\")\n",
    "ax2.set_title('With Clipping')\n",
    "ax2.set_xlabel('Balance Duration (T)')\n",
    "ax2.set_ylabel('Reward')\n",
    "ax2.grid(True)\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Build Neural Network controller (action network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cart Pole and BPTT implementation using Keras.  \n",
    "# Written for tensorflow v.2.2\n",
    "# Author: m.fairbank@essex.ac.uk.  July 2020\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "batchSize=4\n",
    "state_dimension=5 # x,xdot, theta, thetaDot, timesteps\n",
    "action_network_num_inputs=5 # \n",
    "action_network_num_outputs=1 \n",
    "\n",
    "\n",
    "\n",
    "class ActionNetwork(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ActionNetwork, self).__init__()\n",
    "        self.layer1=layers.Dense(6, activation='tanh', input_shape=(action_network_num_inputs,))\n",
    "        self.layer2=layers.Dense(6, activation='tanh')\n",
    "        self.output_layer=layers.Dense(action_network_num_outputs, activation='tanh')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, input_vector):\n",
    "        x=input_vector\n",
    "        y=self.layer1(x)\n",
    "        x=tf.concat([x,y], axis=1)# This adds shortcut connections from the previous layer to the next layer\n",
    "        y=self.layer2(x)\n",
    "        x=tf.concat([x,y], axis=1)# More shortcut connections.\n",
    "        y=self.output_layer(x)\n",
    "        # Using the shortcut connections above means I don't need to worry \n",
    "        # too much about how many hidden layers to add.  For example, if hidden \n",
    "        # layers 1 and 2 are not needed then they can simply be skipped over.\n",
    "        # Also it ensures there are shortcut connections from the input layer to the final layer, which\n",
    "        # potentially allows memories in memory cells to be preserved better.\n",
    "        return y\n",
    "\n",
    "\n",
    "def convert_state_to_action_network_input_vector(state):\n",
    "    return state*tf.constant([1.0/0.16,1,15.0/math.pi, 4,1.0/300.0],tf.float32)\n",
    "\n",
    "keras_action_network=ActionNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Physics model\n",
    "Look in the code for the physics model.\n",
    "These equations can be found in, among other papers,\n",
    "- Florian, Razvan V. \"Correct equations for the dynamics of the cart-pole system.\" Center for Cognitive and Neural Studies (Coneural), Romania (2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# standard cart-pole constants:\n",
    "GRAVITY = 9.8\n",
    "MASSCART = 1.0\n",
    "MASSPOLE = 0.1\n",
    "TOTAL_MASS = (MASSPOLE + MASSCART)\n",
    "LENGTH = 0.5 # // actually half the pole's length\n",
    "POLEMASS_LENGTH = (MASSPOLE * LENGTH)\n",
    "FORCE_MAG = 10.0\n",
    "TAU = 0.02*2 # seconds between state updates.\n",
    "FOURTHIRDS = 1.3333333333333\n",
    "XMAX = 2.4\n",
    "THETAMAX = 12.0 / 180 * math.pi\n",
    "\n",
    "trajectory_length=50 # This is the target balance duration.  Can increase it to 300 for a greater challenge\n",
    "\n",
    "def run_physics_model_one_step(state, action, trajectories_terminated):\n",
    "    x=state[:,0]\n",
    "    xdot=state[:,1]\n",
    "    theta=state[:,2]\n",
    "    thetadot=state[:,3]\n",
    "    time_steps_elapsed=state[:,4]\n",
    "\n",
    "    thrust=tf.reshape(action,[-1])\n",
    "\n",
    "    # implement standard cart-pole physics model\n",
    "    # See Florian, Razvan V. \"Correct equations for the dynamics of the cart-pole system.\" \n",
    "    # Center for Cognitive and Neural Studies (Coneural), Romania (2007)\n",
    "    force = (thrust) * FORCE_MAG;\n",
    "    costheta = tf.cos(theta)\n",
    "    sintheta = tf.sin(theta)\n",
    "    temp = (force + POLEMASS_LENGTH * tf.square(thetadot) * sintheta) / TOTAL_MASS\n",
    "    thetaacc = (GRAVITY * sintheta - costheta * temp) / (LENGTH * (FOURTHIRDS - MASSPOLE * costheta * costheta / TOTAL_MASS))\n",
    "    xacc = temp - POLEMASS_LENGTH * thetaacc * costheta / TOTAL_MASS\n",
    "    \n",
    "    # Use Euler method to advance forwards in time by amount TAU\n",
    "    new_theta=theta+thetadot*TAU\n",
    "    new_x=x+xdot*TAU\n",
    "    \n",
    "    # implement clipping (necessary to make cartpole work with BPTT - see Fairbank et al 2014, \n",
    "    # \"Clipping in Neurocontrol by Adaptive Dynamic Programming\")\n",
    "    excess_theta=tf.maximum(tf.abs(new_theta)-THETAMAX,0)\n",
    "    excess_x=tf.maximum(tf.abs(new_x)-XMAX,0)\n",
    "    time_to_subract_off_due_to_x_clipping=tf.where(excess_x>0,tf.abs(safe_divide(excess_x,xdot)),tf.zeros_like(excess_x))# all elements shuold be >=0\n",
    "    time_to_subract_off_due_to_theta_clipping=tf.where(excess_theta>0,tf.abs(safe_divide(excess_theta,thetadot)),tf.zeros_like(excess_theta))# all elements shuold be >=0\n",
    "    time_to_subtract_off_due_to_clipping=tf.maximum(time_to_subract_off_due_to_x_clipping, time_to_subract_off_due_to_theta_clipping)/TAU\n",
    "    timestep_sizes=(1-time_to_subtract_off_due_to_clipping)# all the values in the vector must be between 0 and 1\n",
    "\n",
    "    next_state=state+tf.stack([xdot*TAU*timestep_sizes, xacc*TAU*timestep_sizes, thetadot*TAU*timestep_sizes, thetaacc*TAU*timestep_sizes, timestep_sizes],axis=1)\n",
    "    trajectories_terminating=tf.logical_or(time_steps_elapsed>=trajectory_length-1,tf.logical_or(excess_theta>0, excess_x>0))\n",
    "    rewards=tf.zeros_like(timestep_sizes) # in cart-pole, we traditionally don't give any reward until the pole falls over.\n",
    "    return [rewards, next_state, timestep_sizes, trajectories_terminating]\n",
    "\n",
    "def evaluate_final_state(state):\n",
    "    time_steps_elapsed=state[:,4]\n",
    "    result=-tf.pow(0.97, time_steps_elapsed) # This is the traditional cart-pole reward function, i.e. zero reward at all time steps until the final step when it receives -1\n",
    "    return result\n",
    "\n",
    "\n",
    "def safe_divide(tensor_numerator, tensor_denominator):\n",
    "    # attempt to avoid NaN bug in tf.where: https://github.com/tensorflow/tensorflow/issues/2540\n",
    "    safe_denominator = tf.where(tf.not_equal(tensor_denominator,tf.zeros_like(tensor_denominator)), tensor_denominator, tensor_denominator+1)\n",
    "    return tensor_numerator/safe_denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluate Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function    \n",
    "def expand_trajectories(start_state):\n",
    "    total_rewards=tf.constant(0.0, dtype=tf.float32, shape=[batchSize]) \n",
    "    state=start_state\n",
    "    trajectory=tf.expand_dims(start_state,axis=0)\n",
    "    trajectories_terminated=tf.cast(tf.zeros_like(start_state[:,0]),tf.bool)\n",
    "\n",
    "    # Evaluate full trajectory.  This while-loop unrolls a set of trajectories through time, \n",
    "    # until the all of the trajectories have terminated.  Each step includes one \n",
    "    # action network evaluation followed by one physics-model evaluation.  As we go \n",
    "    # along the trajectory, we accumulate the total reward.\n",
    "    \n",
    "    [state,total_rewards,trajectory,trajectories_terminated]=tf.while_loop(while_loop_cond, while_loop_body, (state,total_rewards,trajectory,trajectories_terminated), shape_invariants=(state.get_shape(), total_rewards.get_shape(),tf.TensorShape([None, state.get_shape()[0], state_dimension]),trajectories_terminated.get_shape()))\n",
    "    average_total_reward=tf.reduce_mean(total_rewards)\n",
    "    return [average_total_reward,trajectory]\n",
    "\n",
    "@tf.function    \n",
    "def while_loop_cond(state,total_rewards,trajectory,trajectories_terminated):\n",
    "    return tf.logical_not(tf.reduce_all(trajectories_terminated))\n",
    "    \n",
    "@tf.function    \n",
    "def while_loop_body(state,total_rewards,trajectory,trajectories_terminated):\n",
    "    action=keras_action_network(convert_state_to_action_network_input_vector(state))\n",
    "    [rewards,next_state,timestep_sizes,trajectories_terminating]=run_physics_model_one_step(state,action,trajectories_terminated)\n",
    "    state=tf.where(tf.expand_dims(trajectories_terminated,1), state, next_state)\n",
    "    total_rewards+=tf.where(trajectories_terminated, tf.zeros_like(rewards), rewards)\n",
    "    total_rewards+=tf.where(tf.logical_and(trajectories_terminating, tf.logical_not(trajectories_terminated)), evaluate_final_state(state), tf.zeros_like(rewards))\n",
    "    trajectories_terminated=tf.logical_or(trajectories_terminated, trajectories_terminating)\n",
    "    trajectory=tf.concat([trajectory, tf.expand_dims(state,axis=0)],axis=0)\n",
    "    return state,total_rewards,trajectory,trajectories_terminated\n",
    "\n",
    "# randomise initial cartpole positions:\n",
    "np.random.seed(1)\n",
    "initial_state=tf.constant( (np.random.rand(batchSize,state_dimension)-0.5)*0.1*[1.0,1.0,1.0,1.0,0.0], tf.float32)#[0,0,math.pi/180*12,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plot trajectories\n",
    "\n",
    "- This will plot the trajectories and show the pole falling over \n",
    "    - (because we haven't trained the network yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "[average_total_reward,trajectories] = expand_trajectories(initial_state)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "def plot_trajectories(trajectory, iteration_number, fig0):\n",
    "    display.clear_output(wait=True)\n",
    "    if fig0!=None:\n",
    "        plt.close(fig0)\n",
    "    #fig=plt.figure(figsize=[12.4, 4.8])\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "    ax1.axis([0,trajectory_length,-THETAMAX*180/math.pi,THETAMAX*180/math.pi])\n",
    "    ax1.set(xlabel='Time Step', ylabel='Theta')\n",
    "    ax1.set_title('Trajectory (Theta).  Iteration '+str(iteration_number))\n",
    "    for traj in range(batchSize):\n",
    "        trajectory_theta_coord=trajectory[:,traj,2]*180/math.pi\n",
    "        ax1.plot(trajectory_theta_coord)\n",
    "    \n",
    "    ax2.axis([0,trajectory_length,-XMAX,XMAX])\n",
    "    ax2.set(xlabel='Time Step', ylabel='x')\n",
    "    ax2.set_title('Trajectory (x). Iteration '+str(iteration_number))\n",
    "    for traj in range(batchSize):\n",
    "        trajectory_x_coord=trajectory[:,traj,0]\n",
    "        ax2.plot(trajectory_x_coord)\n",
    "    if fig0!=None:\n",
    "        display.display(plt.gcf())\n",
    "    return fig\n",
    "fig=None\n",
    "plot_trajectories(trajectories,0,None)\n",
    "duration_history=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "#optimizer=keras.optimizers.SGD(0.01)\n",
    "\n",
    "for iteration in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the trajectory.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        [average_total_reward,trajectory] = expand_trajectories(initial_state) \n",
    "        loss=-average_total_reward\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss, keras_action_network.trainable_weights) # The \"back-propagation through time\" calculation is the computation of this gradient\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, keras_action_network.trainable_weights))\n",
    "\n",
    "    final_trajectory_steps=trajectory[-1,:]\n",
    "    mean_balancing_duration=np.mean(final_trajectory_steps[:,4])\n",
    "    duration_history=duration_history+[mean_balancing_duration]\n",
    "    #print(\"iteration:\",len(duration_history), \"Num steps balanced:%.13f\"%mean_balancing_duration)\n",
    "    if (iteration%10)==0: \n",
    "        fig=plot_trajectories(trajectory, len(duration_history),fig)\n",
    "        if mean_balancing_duration==trajectory_length:\n",
    "            print(\"(Solved)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_trajectories(trajectory, len(duration_history),None)\n",
    "plt.figure()\n",
    "plt.plot(duration_history)\n",
    "plt.ylabel('Mean Trajectory Duration before Failure')\n",
    "plt.xlabel('Iterations')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments to try:\n",
    "\n",
    "- Try running the main learning process several times (from scratch) to see how reliable it is.\n",
    "- Try increasing the trajectory length to 200 or 300 to see if it will still converge to a solution.\n",
    "- See if you can work out how to switch off clipping\n",
    "    - if you achieve this then this learning algorithm should fail at this problem.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "- Reinforcement Learning (RL) is a closely related and complex topic.  \n",
    "    - Unlike what we did above, RL does not allow for differentiation through the physics model\n",
    "    - See https://keras.io/examples/rl/actor_critic_cartpole/ for an example of cart pole being solved with a model-free algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
