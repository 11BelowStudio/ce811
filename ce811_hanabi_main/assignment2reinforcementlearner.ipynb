{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# An extremely ill-advised attempt at making a reinforcement learner for assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from hanabi_learning_environment import rl_env\n",
    "from hanabi_learning_environment.rl_env import Agent\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def calculate_reinforce_gradient1(trajectory, total_reward, baseline, action_choices):\n",
    "    # This function is meant to calculate (dL/d Theta), where L=(\\sum_t (log(P_t))(R-b).\n",
    "    # You need to use the functions tf.math.log and tf.reduce_sum in here, plus the multiplication and subtraction operations.\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(table_action_probabilities_before_softmax)\n",
    "        trajectory_action_probabilities=get_action_probabilities_for_trajectory(trajectory) # this returns a tensor of shape [trajectory_length,4]\n",
    "        chosen_probabilities=tf.gather(trajectory_action_probabilities, indices=action_choices, axis=1, batch_dims=1) # this returns a tensor of shape [trajectory_length]\n",
    "        log_probabilities=tf.math.log(chosen_probabilities)\n",
    "        L=tf.reduce_sum(log_probabilities)*(total_reward-baseline)\n",
    "    assert len(L.shape)==0 # checking the original large array has gone through a reduce_sum\n",
    "    grads = tape.gradient(L, table_action_probabilities_before_softmax) # This calculates the gradient required by REINFORCE\n",
    "    # This function doesn't actually do the update.  It just calculates the gradient ascent direction, and returns it!\n",
    "    return grads\n",
    "\n",
    "def calculate_reinforce_gradient(episode_observations, rewards_minus_baseline, episode_actions, policy_network):\n",
    "    # This function is meant to calculate (dL/d Theta), where L=(\\sum_t (log(P_t))(R-b).\n",
    "\n",
    "    # Train on episode\n",
    "    batch_trajectory=np.stack(episode_observations)\n",
    "    batch_action_choices=np.stack(episode_actions).astype(np.int32)\n",
    "\n",
    "    # Check all input arrays are the correct shape...\n",
    "    assert batch_trajectory.shape[0]==rewards_minus_baseline.shape[0]\n",
    "    assert batch_trajectory.shape[0]==batch_action_choices.shape[0]\n",
    "    assert len(batch_trajectory.shape)==2\n",
    "    assert len(rewards_minus_baseline.shape)==1\n",
    "    assert len(batch_action_choices.shape)==1\n",
    "\n",
    "    # This is the REINFORCE gradient calculation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Note, don't need a tape.watch here because tensorflow by default always \"watches\" all Variable tensors, i.e. all of our neural network weights.\n",
    "        trajectory_action_probabilities=policy_network(batch_trajectory)\n",
    "        # Note that the next 2 lines could be repaced by a single call to tf.keras.losses.SparseCategoricalCrossentropy\n",
    "        chosen_probabilities=tf.gather(trajectory_action_probabilities,indices=batch_action_choices,axis=1, batch_dims=1) # this returns a tensor of shape [trajectory_length]\n",
    "        log_probabilities=tf.math.log(chosen_probabilities)\n",
    "        logprobrewards=log_probabilities*rewards_minus_baseline # Instead of using R-baseline, we are using R_t-baseline here,\n",
    "                                                                # i.e. where R_t is the reward to go from step t\n",
    "        L=tf.reduce_sum(logprobrewards)\n",
    "    assert len(L.shape)==0 # checking the original large array has gone through a reduce_sum\n",
    "    grads = tape.gradient(L, policy_network.trainable_weights) # This calculates the gradient required by REINFORCE\n",
    "    # This function doesn't actually do the update.  It just calculates the gradient ascent direction, and returns it!\n",
    "    return grads\n",
    "\n",
    "\n",
    "def calculate_accumulated_discounted_rewards(episode_rewards, discount_factor) -> np.ndarray:\n",
    "    #discounted_episode_rewards= np.ones_like(episode_rewards)*sum(episode_rewards)\n",
    "\n",
    "    discounts = list(discount_factor ** i for i in range(len(episode_rewards)))\n",
    "\n",
    "    return np.array(\n",
    "        list(\n",
    "            sum(episode_rewards[i + j] * (discounts[j]) for j in range(0, len(episode_rewards) - i))\n",
    "            for i in range(len(episode_rewards))\n",
    "        )\n",
    "    )\n",
    "    # We need to return a numpy array of the same length and shape as the input array episode_rewards.\n",
    "\n",
    "\n",
    "def run_stochastic_policy(policy_network, observation):\n",
    "    # Reshape observation to (1,num_features)\n",
    "    observation = observation[np.newaxis,:]\n",
    "    # Run forward propagation to get softmax probabilities\n",
    "    action_probabilities = policy_network(observation).numpy().reshape(-1)\n",
    "    # Select action using a biased sample\n",
    "    # this will return the index of the action we've sampled\n",
    "    action = np.random.choice(range(len(action_probabilities)), p=action_probabilities)\n",
    "    return action\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class MyAgent(Agent):\n",
    "\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        self.config = config\n",
    "        # Extract max info tokens or set default to 8.\n",
    "        self.max_information_tokens: int = config.get('information_tokens', 8)\n",
    "        pass\n",
    "\n",
    "    def reset(self, config):\n",
    "        pass\n",
    "\n",
    "    def act(self, observation):\n",
    "        if observation['current_player_offset'] != 0:\n",
    "            return None # No action returned (because it's not our turn!)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}