{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE811 Tabular Q-learning Example\n",
    "\n",
    "M. Fairbank, University of Essex, December 2021\n",
    "\n",
    "- In this lab we use a table of learnable Q-values with the Q-learning algorithm to solve a maze.\n",
    "\n",
    "- The environment gives a reward of -1 every time step, so to maximise reward, the agent must learn to finish the maze as fast as possible.\n",
    "\n",
    "- Q-learning is an on-line algorithm, so the agent can solve the maze on its very first attempt (if we wait long enough)\n",
    "\n",
    "Acknowledgements:  This Q-learning and maze environment is based on one initially built by M. Pisheh (University of Essex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First build a maze environment:\n",
    "- The maze is simply a numpy array.  0s represent walkable areas.  1s represent solid walls.\n",
    "- The environment_step function is the function that executes an agent's action (i.e north/south/east/west) \n",
    "- It calculates the new state (y,x) and the instantaneous reward (-1 each step).\n",
    "- Note that throughout this notebook, states are y,x not x,y!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "maze=np.array([\n",
    "        [1,1,1,1,1,1,1,1,1],\n",
    "        [1,0,1,0,0,0,0,0,1],\n",
    "        [1,0,1,1,1,0,1,0,1],\n",
    "        [1,0,0,0,1,0,1,0,1],\n",
    "        [1,1,1,0,1,0,1,0,1],\n",
    "        [1,0,0,0,1,0,1,0,1],\n",
    "        [1,0,1,1,1,0,1,1,1],\n",
    "        [1,0,1,0,0,0,0,0,1],\n",
    "        [1,0,1,1,1,1,1,0,1],\n",
    "        [1,0,0,0,0,0,0,0,1],\n",
    "        [1,1,1,1,1,1,1,0,1],\n",
    "        [1,0,0,0,0,0,1,0,1],\n",
    "        [1,0,1,0,1,1,1,0,1],\n",
    "        [1,0,1,0,0,0,1,0,1],\n",
    "        [1,0,1,1,1,0,1,0,1],\n",
    "        [1,0,0,0,1,0,0,0,1],\n",
    "        [1,1,1,1,1,1,1,1,1]])\n",
    "\n",
    "maze_width=maze.shape[1]\n",
    "maze_height=maze.shape[0]\n",
    "start_state = [1,1] # top left corner zero\n",
    "goal_state = [maze_height-2,maze_width-2] # bottom-right corner zero\n",
    "\n",
    "action_names=[\"North\",\"South\",\"West\",\"East\"]\n",
    "action_effects=[[-1,0],[1,0],[0,-1],[0,1]]\n",
    "\n",
    "def environment_step(action,state):\n",
    "    y,x = state\n",
    "    dy,dx=action_effects[action]\n",
    "    new_x = x+dx\n",
    "    new_y = y+dy\n",
    "    if new_x <0 or new_x>=maze_width:\n",
    "        # off grid\n",
    "        new_x = x\n",
    "    if new_y <0 or new_y>=maze_height:\n",
    "        # off grid\n",
    "        new_y = y\n",
    "    if maze[new_y,new_x] == 1:\n",
    "        # hit wall\n",
    "        new_y=y\n",
    "        new_x=x\n",
    "    new_state = [new_y,new_x]\n",
    "    reward = -1\n",
    "    done = (new_state==goal_state)\n",
    "    return new_state, reward, done\n",
    "\n",
    "print(\"maze\",maze)\n",
    "print(\"start\",start_state)\n",
    "print(\"goal\",goal_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next build our table of Q-values. \n",
    "- There are 4 potential actions from each maze cell, therefore we need 4 Q-values for each maze cell, i.e. a numpy array of shape [maze_height, maze_width, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our table of Q-values.  We need 4 q-values for every cell in the maze.\n",
    "Qtable = np.zeros((maze_height, maze_width, 4), dtype=np.float64)\n",
    "print(Qtable.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next define a policy function\n",
    "- This is the epsilon greedy policy.\n",
    "- Remember, Q-learning has 2 policies\n",
    "- This policy (the epsilon-greedy policy) is the policy the agent will follow.\n",
    "- <span style=\"color:red\">TODO: modify the code below to the epsilon-greedy policy</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(current_state, epsilon):\n",
    "    # This function runs the epsilon-greedy policy to choose the integer action to take (i.e. 0,1,2 or 3)\n",
    "    # TODO modify this function to be the epsilon greedy policy.\n",
    "    # With probability epsilon, it should return a purely random action.\n",
    "    \n",
    "    # Greedy policy\n",
    "    ??? NOT FINISHED\n",
    "    y,x=current_state\n",
    "    q_values=Qtable[y,x,:]\n",
    "    best_q_value=q_values.max()\n",
    "    best_q_indices=np.argwhere(q_values == best_q_value).flatten().tolist()\n",
    "    #if multiple q values have the maximum value, then choose one of them randomly\n",
    "    choice = np.random.choice(best_q_indices)\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Define the Q-learning update (learning step)\n",
    "- Remember for Q-learning, the update is $\\Delta Q_t=\\eta (Q'_t - Q_t)$ where\n",
    "$$Q_t=Qtable[s_t,a_t]$$ and\n",
    "$$Q'_t=\\begin{cases}r_t + \\gamma \\max_{a}Q(s_{t+1},a)&\\text{if }s_{t+1}\\notin \\mathbb{T}\\\\r_t&\\text{if }s_{t+1} \\in \\mathbb{T}\\end{cases}$$\n",
    "- Note in the function arguments below:\n",
    "  - state is $s_t$\n",
    "  - action is $a_t$\n",
    "  - reward is $r_t$\n",
    "  - next_state is $s_{t+1}$\n",
    "  - done is True iff $s_{t+1} \\in \\mathbb{T}$\n",
    "  - discount_factor is $\\gamma$\n",
    "- Also note that each state $s_t$ and $s_{t+1}$ can be decomposed into $(y,x)$.  It's NOT $(x,y)$.\n",
    "- <span style=\"color:red\">TODO: modify the code below to apply the q-learning update...</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_q_update(state, action, reward, next_state, done, discount_factor):\n",
    "    sy,sx=state\n",
    "    nsy,nsx=next_state\n",
    "    current_q_value = Qtable[sy,sx,action]\n",
    "    all_q_values_at_next_state=Qtable[nsy,nsx, :]\n",
    "    # TODO set the variable target_q_value here....\n",
    "    if done:\n",
    "        target_q_value = ???\n",
    "    else:\n",
    "        # Hint use \"all_q_values_at_next_state\" and \"np.max\" in this next line.\n",
    "        target_q_value = ???\n",
    "    Qtable[sy,sx,action] += learning_rate * (target_q_value- current_q_value) #update\n",
    "    return current_q_value, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the main trajectory unroll loop, and learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "learning_rate = 0.1\n",
    "epsilon_greedy = 0.1\n",
    "discount_factor=1\n",
    "\n",
    "reward_history=[]\n",
    "trajectory_length_history=[]\n",
    "for iteration in range(iterations):\n",
    "    state=start_state\n",
    "    total_reward=0\n",
    "    done=False\n",
    "    time_step=0\n",
    "    while not done:\n",
    "        # Choose an action\n",
    "        action = run_policy(state, epsilon_greedy)\n",
    "        #print(\"time_step\",time_step,\"state\",state,\"action\",action)\n",
    "        next_state, reward, done = environment_step(action, state)\n",
    "        #print(\"action\",action, \"next_state\", next_state, \"reward\",reward, \"done\", done)\n",
    "        \n",
    "        apply_q_update(state, action, reward, next_state,  done, discount_factor)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward*(discount_factor**time_step)\n",
    "        time_step+=1\n",
    "        if time_step>50000:\n",
    "            raise Exception(\"Should have solved it by now - something is wrong with the code\")\n",
    "    print(\"Iteration\",iteration,\"Done.  Total_reward=\",total_reward, \"Trajectory length\",time_step)\n",
    "    reward_history.append(total_reward)\n",
    "    trajectory_length_history.append(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphs\n",
    "- Should show performance improving over time....\n",
    "- This maze is theoretically solvable in 24 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(trajectory_length_history)\n",
    "plt.ylabel('Trajectory Length')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements?\n",
    "\n",
    "- We could use a decaying epsilon greedy policy to make the trajectory length converge to the optimal length.\n",
    "- If you experiment with setting epsilon to zero then it sometimes still works really well.  This is very unusual.  It must because the default Q-values of zero are higher than the final Q-values (which are all negative) therefore exploration is encouraged towards rarely-visited locations.  But have a think about this.\n",
    "- A GUI showing the agent in the maze would be nice, but animations will slow down learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
