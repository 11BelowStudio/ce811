{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE811 Tabular REINFORCE Example\n",
    "\n",
    "M. Fairbank, University of Essex, December 2021\n",
    "\n",
    "- In this lab we use a table of action probabilities (one row for each cell of a maze) with the REINFORCE algorithm to solve a maze.\n",
    "\n",
    "- REINFORCE is a policy-gradient algorithm.  Unlike Q-learning, it requires completed trajectories to work, so we force a trajectory-cutoff after 200 time steps.  \n",
    "\n",
    "- For this reason, we also require a smaller simpler maze to get this algorithm to work in a reasonable time.\n",
    "\n",
    "- In this notebook, there are just two \"TODO\" parts for you to complete.  Do these, and read through all of the code the code, run it, plot the graph; and try to understand it!\n",
    "\n",
    "Acknowledgements: This maze environment is based on one initially built by M. Pisheh (University of Essex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First build a maze environment (as in previous example):\n",
    "- The maze is simply a numpy array.  0s represent walkable areas.  1s represent solid walls.\n",
    "- The environment_step function is the function that executes an agent's action (i.e north/south/east/west) \n",
    "- It calculates the new state (y,x) and the instantaneous reward (-1 each step).\n",
    "- Note that throughout this notebook, states are y,x not x,y!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze [[1 1 1 1 1 1 1]\n",
      " [1 0 1 0 0 0 1]\n",
      " [1 0 1 1 1 0 1]\n",
      " [1 0 0 0 0 0 1]\n",
      " [1 1 1 0 1 0 1]\n",
      " [1 0 0 0 1 0 1]\n",
      " [1 1 1 1 1 1 1]]\n",
      "start [1, 1]\n",
      "goal [5, 5]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "maze=np.array([\n",
    "        [1,1,1,1,1,1,1],\n",
    "        [1,0,1,0,0,0,1],\n",
    "        [1,0,1,1,1,0,1],\n",
    "        [1,0,0,0,0,0,1],\n",
    "        [1,1,1,0,1,0,1],\n",
    "        [1,0,0,0,1,0,1],\n",
    "        [1,1,1,1,1,1,1]])\n",
    "\n",
    "maze_width=maze.shape[1]\n",
    "maze_height=maze.shape[0]\n",
    "start_state = [1,1] # top left corner zero\n",
    "goal_state = [maze_height-2,maze_width-2] # bottom-right corner zero\n",
    "\n",
    "action_names=[\"North\",\"South\",\"West\",\"East\"]\n",
    "action_effects=[[-1,0],[1,0],[0,-1],[0,1]]\n",
    "\n",
    "def environment_step(action,state):\n",
    "    y,x = state\n",
    "    dy,dx=action_effects[action]\n",
    "    new_x = x+dx\n",
    "    new_y = y+dy\n",
    "    if new_x <0 or new_x>=maze_width:\n",
    "        # off grid\n",
    "        new_x = x\n",
    "    if new_y <0 or new_y>=maze_height:\n",
    "        # off grid\n",
    "        new_y = y\n",
    "    if maze[new_y,new_x] == 1:\n",
    "        # hit wall\n",
    "        new_y=y\n",
    "        new_x=x\n",
    "    new_state = [new_y,new_x]\n",
    "    reward = -1\n",
    "    done = (new_state==goal_state)\n",
    "    return new_state, reward, done\n",
    "\n",
    "print(\"maze\",maze)\n",
    "print(\"start\",start_state)\n",
    "print(\"goal\",goal_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next build our table of Probabilities for action in each maze cell. \n",
    "- There are 4 potential actions from each maze cell, therefore we need 4 probabilities for each maze cell.\n",
    "- Since the maze itself is shape [maze_height, maze_width], therefore we need an array of shape [maze_height, maze_width, 4]\n",
    "- Since in this table, every number is a completely free parameter, hence each row probably will not add up to 1 like probabilities do.  Hence we will need to remember to always use it in conjunction with softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create our table of action probabilities.  We need 4 probabilities for every cell in the maze. \n",
    "table_action_probabilities_before_softmax=tf.Variable(np.zeros((maze_height,maze_width,len(action_names)),np.float32),tf.float32)\n",
    "print(table_action_probabilities_before_softmax.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next define a stochastic policy function\n",
    "- The policy gets probabilities directly from the table and the softmax function.\n",
    "- Once the 4 probabilities are known, it samples one of them at random.\n",
    "\n",
    "\n",
    "**TODO: Finish the *run_stochastic_policy* function below.**  \n",
    "- It needs to choose an action randomly, given the corresponding numpy array of probabilities.\n",
    "- So for example, if the array of probabilities is [0.1,0.3,0.2,0.4] then it needs to return 0 with probability 0.1, or return 1 with probability 0.3, or return 2 with probability 0.2, or return 3 with probability 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_probabilities(state):\n",
    "    y=state[0]\n",
    "    x=state[1]\n",
    "    # We can't just use a value straight from the table, because it might not be a valid probability between 0 and 1.\n",
    "    # Hence we still need to put all 4 probabilities through a softmax function to ensure they are all \n",
    "    # positive and sum to 1....\n",
    "    return tf.nn.softmax(table_action_probabilities_before_softmax[y,x,:])\n",
    "\n",
    "def run_stochastic_policy(current_state):\n",
    "    # Choose an action for current_state \n",
    "    #probabilities=get_action_probabilities(current_state).numpy() # The .numpy() here converts from tensorflow tensor to a numpy array\n",
    "    #action_chosen = np.random.choice([0,1,2,3], 1, probabilities)\n",
    "    # TODO write code here to set action_chosen to be 0,1,2 or 3, chosen randomly with probabilities given by the probabilities array.\n",
    "    #return action_chosen\n",
    "    return np.random.choice([0,1,2,3], 1, p=get_action_probabilities(current_state).numpy())[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Define the REINFORCE update\n",
    "- Remember for REINFORCE, the update is $\\Delta \\theta=\\eta \\left(\\sum_t \\frac{dlog(P_t)}{d\\theta}\\right) (R-b)$ where $P_t$ is the probability assigned to the action that was actually chosen at time step $t$, (i.e. $P_t= p(a_t)$). We saw the proof in the lecture that $\\mathbb{E}(\\Delta \\theta)=\\eta \\frac{d\\mathbb{E}(R)}{d\\theta}$, i.e. that the expectation (average) of the REINFORCE update gives us gradient ascent on the expectation of total trajectory reward $R$.\n",
    "\n",
    "To work out $P_t$ for each trajectory step, we can use the following two lines of code:\n",
    "\n",
    "```\n",
    "trajectory_action_probabilities=get_action_probabilities_for_trajectory(trajectory) # this returns a tensor of shape [trajectory_length,4]\n",
    "chosen_probabilities=tf.gather(trajectory_action_probabilities, indices=action_choices, axis=1, batch_dims=1) # this returns a tensor of shape [trajectory_length]\n",
    "```\n",
    "\n",
    "This code is already given to you in the code block below.  When this code is run, it will create an array *chosen_probabilities* of shape [trajectory_length], and the array's content will be [$P_0$,$P_1$,...,$P_n$].  If you want more detail then see the help on [tf.gather](https://www.tensorflow.org/api_docs/python/tf/gather) to understand this more.\n",
    "\n",
    "- To compute the derivative used by REINFORCE, we will use auto-differentiation, because, remember our probabilities came from a softmax function,  and differentiating by hand through the softmax function might be beyond the scope of this course.  So we'll let tensorflow do the hard work for us!...\n",
    "- To calculate $\\left(\\sum_t \\frac{dlog(P_t)}{d\\theta}\\right) (R-b)$, we will first simplify things a little, by writing,\n",
    "\\begin{align}&\\left(\\sum_t \\frac{dlog(P_t)}{d\\theta}\\right) (R-b)\\\\=&\\frac{d}{d\\theta}\\left[\\left(\\sum_t log(P_t)\\right)(R-b)\\right]\\\\\n",
    "=&\\frac{dL}{d\\theta}\\end{align}\n",
    "where we define:  \\begin{align}L=\\left(\\sum_t log(P_t)\\right)(R-b)\\end{align}\n",
    "- Now we can let tensorflow just differentiate $L$, to obtain the gradient $\\frac{dL}{d\\theta}$.  This will enable us to do the weight update $\\Delta \\theta = \\eta \\frac{dL}{d\\theta}$.\n",
    "- Here $\\theta$ is our \"parameter vector\", by which we mean the learnable quantities.  So this is the variable *table_action_probabilities_before_softmax* in our case. \n",
    "- Hence we need to set up a gradient tape, and within it define $L=\\left(\\sum_t log(P_t)\\right)(R-b)$, and then use the gradient tape to compute $\\frac{dL}{d\\theta}$ for us, where $\\theta$ means table_action_probabilities_before_softmax.  You need to complete the following code to do that.  You will need to use the tensorflow functions tf.math.log and tf.reduce_sum in here, and the multiplication and subraction operations.  You will also need to use the variables chosen_probabilities, total_reward, and baseline.\n",
    "\n",
    "**TODO: finish the GradientTape code block below...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_probabilities_for_trajectory(trajectory):\n",
    "    # On entry, trajectory is a tensor of shape [trajectory_length,2]\n",
    "    # This returns a tensor of shape [trajectory_length,4]\n",
    "    # Each row of this tensor is an array of 4 probabilities applicable to timestep t\n",
    "    trajectory_length=len(trajectory)\n",
    "    # (These next 2 line could be made much more efficient by operating on whole trajectory at once!...)\n",
    "    trajectory_action_probabilities=[get_action_probabilities(trajectory[t,:]) for t in range(trajectory_length)]\n",
    "    return tf.stack(trajectory_action_probabilities) # converts the list to a tensor\n",
    "\n",
    "def calculate_reinforce_gradient(trajectory, total_reward, baseline, action_choices):\n",
    "    # This function is meant to calculate (dL/d Theta), where L=(\\sum_t (log(P_t))(R-b).\n",
    "    # You need to use the functions tf.math.log and tf.reduce_sum in here, plus the multiplication and subtraction operations.  \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(table_action_probabilities_before_softmax)\n",
    "        trajectory_action_probabilities=get_action_probabilities_for_trajectory(trajectory) # this returns a tensor of shape [trajectory_length,4]\n",
    "        chosen_probabilities=tf.gather(trajectory_action_probabilities, indices=action_choices, axis=1, batch_dims=1) # this returns a tensor of shape [trajectory_length]\n",
    "        L = tf.reduce_sum(tf.math.log(chosen_probabilities)) * (total_reward - baseline)\n",
    "        #L=0 # TODO fix this line of code!\n",
    "    assert len(L.shape)==0 # checking the original large array has gone through a reduce_sum\n",
    "    grads = tape.gradient(L, table_action_probabilities_before_softmax) # This calculates the gradient required by REINFORCE\n",
    "    # This function doesn't actually do the update.  It just calculates the gradient ascent direction, and returns it!\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the main trajectory unroll loop, and learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 1 total_reward 0.04157799358572413 trajectory_length 63\n",
      "Iteration 2 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 3 total_reward 0.014905125382474753 trajectory_length 83\n",
      "Iteration 4 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 5 total_reward 0.0033675975668514537 trajectory_length 112\n",
      "Iteration 6 total_reward 0.02134373384587751 trajectory_length 76\n",
      "Iteration 7 total_reward 0.010408804957535737 trajectory_length 90\n",
      "Iteration 8 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 9 total_reward 0.043766309037604346 trajectory_length 62\n",
      "Iteration 10 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 11 total_reward 0.013451875657683464 trajectory_length 85\n",
      "Iteration 12 total_reward 0.0005593019998111216 trajectory_length 147\n",
      "Iteration 13 total_reward 0.03217225885613059 trajectory_length 68\n",
      "Iteration 14 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 15 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 16 total_reward 0.14989025404881545 trajectory_length 38\n",
      "Iteration 17 total_reward 0.020276547153583634 trajectory_length 77\n",
      "Iteration 18 total_reward 0.00045555497448365723 trajectory_length 151\n",
      "Iteration 19 total_reward 0.009393946474176 trajectory_length 92\n",
      "Iteration 20 total_reward 0.020276547153583634 trajectory_length 77\n",
      "Iteration 21 total_reward 0.11598222130000556 trajectory_length 43\n",
      "Iteration 22 total_reward 0.04157799358572413 trajectory_length 63\n",
      "Iteration 23 total_reward 0.001408061023535215 trajectory_length 129\n",
      "Iteration 24 total_reward 0.001915489806775088 trajectory_length 123\n",
      "Iteration 25 total_reward 0.23782688525533216 trajectory_length 29\n",
      "Iteration 26 total_reward 0.00988836470965895 trajectory_length 91\n",
      "Iteration 27 total_reward 0.19371148445850087 trajectory_length 33\n",
      "Iteration 28 total_reward 0.0037314100463728024 trajectory_length 110\n",
      "Iteration 29 total_reward 0.02134373384587751 trajectory_length 76\n",
      "Iteration 30 total_reward 0.05953855510552941 trajectory_length 56\n",
      "Iteration 31 total_reward 0.023649566588229934 trajectory_length 74\n",
      "Iteration 32 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 33 total_reward 3.50526662488287e-05 trajectory_length 201\n",
      "Iteration 34 total_reward 0.22593554099256555 trajectory_length 30\n",
      "Iteration 35 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 36 total_reward 0.21463876394293727 trajectory_length 31\n",
      "Iteration 37 total_reward 0.0005887389471696016 trajectory_length 146\n",
      "Iteration 38 total_reward 0.053733545982740286 trajectory_length 58\n",
      "Iteration 39 total_reward 0.027583690436774964 trajectory_length 71\n",
      "Iteration 40 total_reward 0.00847803669294384 trajectory_length 94\n",
      "Iteration 41 total_reward 0.13527595427905592 trajectory_length 40\n",
      "Iteration 42 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 43 total_reward 0.02134373384587751 trajectory_length 76\n",
      "Iteration 44 total_reward 0.12851215656510312 trajectory_length 41\n",
      "Iteration 45 total_reward 0.01829958380610923 trajectory_length 79\n",
      "Iteration 46 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 47 total_reward 0.16608338398760716 trajectory_length 36\n",
      "Iteration 48 total_reward 0.0027429292656853013 trajectory_length 116\n",
      "Iteration 49 total_reward 0.043766309037604346 trajectory_length 62\n",
      "Iteration 50 total_reward 0.053733545982740286 trajectory_length 58\n",
      "Iteration 51 total_reward 0.23782688525533216 trajectory_length 29\n",
      "Iteration 52 total_reward 0.174824614723797 trajectory_length 35\n",
      "Iteration 53 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 54 total_reward 0.12208654873684796 trajectory_length 42\n",
      "Iteration 55 total_reward 0.001089530778848288 trajectory_length 134\n",
      "Iteration 56 total_reward 0.024894280619189402 trajectory_length 73\n",
      "Iteration 57 total_reward 0.07694497527671315 trajectory_length 51\n",
      "Iteration 58 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 59 total_reward 0.13527595427905592 trajectory_length 40\n",
      "Iteration 60 total_reward 0.0156896056657629 trajectory_length 82\n",
      "Iteration 61 total_reward 0.14989025404881545 trajectory_length 38\n",
      "Iteration 62 total_reward 0.1577792147882268 trajectory_length 37\n",
      "Iteration 63 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 64 total_reward 0.0005047700548295372 trajectory_length 149\n",
      "Iteration 65 total_reward 0.27738957312183377 trajectory_length 26\n",
      "Iteration 66 total_reward 0.056561627350252934 trajectory_length 57\n",
      "Iteration 67 total_reward 0.03564793225056022 trajectory_length 66\n",
      "Iteration 68 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 69 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 70 total_reward 0.2039068257457904 trajectory_length 32\n",
      "Iteration 71 total_reward 0.19371148445850087 trajectory_length 33\n",
      "Iteration 72 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 73 total_reward 0.0852575903343082 trajectory_length 49\n",
      "Iteration 74 total_reward 0.030563645913324063 trajectory_length 69\n",
      "Iteration 75 total_reward 0.0030392568040834367 trajectory_length 114\n",
      "Iteration 76 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 77 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 78 total_reward 0.11598222130000556 trajectory_length 43\n",
      "Iteration 79 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 80 total_reward 0.03564793225056022 trajectory_length 66\n",
      "Iteration 81 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 82 total_reward 0.22593554099256555 trajectory_length 30\n",
      "Iteration 83 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 84 total_reward 0.18402591023557582 trajectory_length 34\n",
      "Iteration 85 total_reward 0.14239574134637467 trajectory_length 39\n",
      "Iteration 86 total_reward 0.0156896056657629 trajectory_length 82\n",
      "Iteration 87 total_reward 0.024894280619189402 trajectory_length 73\n",
      "Iteration 88 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 89 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 90 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 91 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 92 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 93 total_reward 0.11598222130000556 trajectory_length 43\n",
      "Iteration 94 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 95 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 96 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 97 total_reward 0.10467395472325501 trajectory_length 45\n",
      "Iteration 98 total_reward 0.13527595427905592 trajectory_length 40\n",
      "Iteration 99 total_reward 0.08974483193085075 trajectory_length 48\n",
      "Iteration 100 total_reward 0.2039068257457904 trajectory_length 32\n",
      "Iteration 101 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 102 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 103 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 104 total_reward 0.23782688525533216 trajectory_length 29\n",
      "Iteration 105 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 106 total_reward 0.27738957312183377 trajectory_length 26\n",
      "Iteration 107 total_reward 0.07309772651287749 trajectory_length 52\n",
      "Iteration 108 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 109 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 110 total_reward 0.23782688525533216 trajectory_length 29\n",
      "Iteration 111 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 112 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 113 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 114 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 115 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 116 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 117 total_reward 0.014905125382474753 trajectory_length 83\n",
      "Iteration 118 total_reward 0.22593554099256555 trajectory_length 30\n",
      "Iteration 119 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 120 total_reward 0.26352009446574204 trajectory_length 27\n",
      "Iteration 121 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 122 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 123 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 124 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 125 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 126 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 127 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 128 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 129 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 130 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 131 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 132 total_reward 0.18402591023557582 trajectory_length 34\n",
      "Iteration 133 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 134 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 135 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 136 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 137 total_reward 0.22593554099256555 trajectory_length 30\n",
      "Iteration 138 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 139 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 140 total_reward 0.323533544973709 trajectory_length 23\n",
      "Iteration 141 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 142 total_reward 0.2503440897424549 trajectory_length 28\n",
      "Iteration 143 total_reward 0.046069798986951946 trajectory_length 61\n",
      "Iteration 144 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 145 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 146 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 147 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 148 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 149 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 150 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 151 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 152 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 153 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 154 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 155 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 156 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 157 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 158 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 159 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 160 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 161 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 162 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 163 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 164 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 165 total_reward 0.174824614723797 trajectory_length 35\n",
      "Iteration 166 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 167 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 168 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 169 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 170 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 171 total_reward 0.323533544973709 trajectory_length 23\n",
      "Iteration 172 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 173 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 174 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 175 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 176 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 177 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 178 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 179 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 180 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 181 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 182 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 183 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 184 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 185 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 186 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 187 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 188 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 189 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 190 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 191 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 192 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 193 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 194 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 195 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 196 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 197 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 198 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 199 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 200 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 201 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 202 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 203 total_reward 0.323533544973709 trajectory_length 23\n",
      "Iteration 204 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 205 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 206 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 207 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 208 total_reward 0.26352009446574204 trajectory_length 27\n",
      "Iteration 209 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 210 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 211 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 212 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 213 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 214 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 215 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 216 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 217 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 218 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 219 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 220 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 221 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 222 total_reward 0.26352009446574204 trajectory_length 27\n",
      "Iteration 223 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 224 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 225 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 226 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 227 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 228 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 229 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 230 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 231 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 232 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 233 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 234 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 235 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 236 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 237 total_reward 0.22593554099256555 trajectory_length 30\n",
      "Iteration 238 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 239 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 240 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 241 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 242 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 243 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 244 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 245 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 246 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 247 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 248 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 249 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 250 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 251 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 252 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 253 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 254 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 255 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 256 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 257 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 258 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 259 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 260 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 261 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 262 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 263 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 264 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 265 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 266 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 267 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 268 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 269 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 270 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 271 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 272 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 273 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 274 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 275 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 276 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 277 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 278 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 279 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 280 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 281 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 282 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 283 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 284 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 285 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 286 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 287 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 288 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 289 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 290 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 291 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 292 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 293 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 294 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 295 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 296 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 297 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 298 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 299 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 300 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 301 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 302 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 303 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 304 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 305 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 306 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 307 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 308 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 309 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 310 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 311 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 312 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 313 total_reward 0.26352009446574204 trajectory_length 27\n",
      "Iteration 314 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 315 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 316 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 317 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 318 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 319 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 320 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 321 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 322 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 323 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 324 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 325 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 326 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 327 total_reward 0.2919890243387724 trajectory_length 25\n",
      "Iteration 328 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 329 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 330 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 331 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 332 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 333 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 334 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 335 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 336 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 337 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 338 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 339 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 340 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 341 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 342 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 343 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 344 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 345 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 346 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 347 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 348 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 349 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 350 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 351 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 352 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 353 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 354 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 355 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 356 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 357 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 358 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 359 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 360 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 361 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 362 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 363 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 364 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 365 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 366 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 367 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 368 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 369 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 370 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 371 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 372 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 373 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 374 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 375 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 376 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 377 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 378 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 379 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 380 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 381 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 382 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 383 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 384 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 385 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 386 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 387 total_reward 0.26352009446574204 trajectory_length 27\n",
      "Iteration 388 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 389 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 390 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 391 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 392 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 393 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 394 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 395 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 396 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 397 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 398 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 399 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 400 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 401 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 402 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 403 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 404 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 405 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 406 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 407 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 408 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 409 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 410 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 411 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 412 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 413 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 414 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 415 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 416 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 417 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 418 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 419 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 420 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 421 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 422 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 423 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 424 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 425 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 426 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 427 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 428 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 429 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 430 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 431 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 432 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 433 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 434 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 435 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 436 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 437 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 438 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 439 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 440 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 441 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 442 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 443 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 444 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 445 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 446 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 447 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 448 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 449 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 450 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 451 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 452 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 453 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 454 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 455 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 456 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 457 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 458 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 459 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 460 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 461 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 462 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 463 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 464 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 465 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 466 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 467 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 468 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 469 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 470 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 471 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 472 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 473 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 474 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 475 total_reward 0.3584859224085419 trajectory_length 21\n",
      "Iteration 476 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 477 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 478 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 479 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 480 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 481 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 482 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 483 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 484 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 485 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 486 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 487 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 488 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 489 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 490 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 491 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 492 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 493 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 494 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 495 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 496 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 497 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 498 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 499 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 500 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 501 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 502 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 503 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 504 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 505 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 506 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 507 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 508 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 509 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 510 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 511 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 512 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 513 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 514 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 515 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 516 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 517 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 518 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 519 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 520 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 521 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 522 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 523 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 524 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 525 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 526 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 527 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 528 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 529 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 530 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 531 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 532 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 533 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 534 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 535 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 536 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 537 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 538 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 539 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 540 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 541 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 542 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 543 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 544 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 545 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 546 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 547 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 548 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 549 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 550 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 551 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 552 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 553 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 554 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 555 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 556 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 557 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 558 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 559 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 560 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 561 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 562 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 563 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 564 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 565 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 566 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 567 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 568 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 569 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 570 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 571 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 572 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 573 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 574 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 575 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 576 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 577 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 578 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 579 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 580 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 581 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 582 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 583 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 584 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 585 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 586 total_reward 0.3073568677250236 trajectory_length 24\n",
      "Iteration 587 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 588 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 589 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 590 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 591 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 592 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 593 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 594 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 595 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 596 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 597 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 598 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 599 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 600 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 601 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 602 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 603 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 604 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 605 total_reward 0.3405616262881148 trajectory_length 22\n",
      "Iteration 606 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 607 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 608 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 609 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 610 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 611 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 612 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 613 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 614 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 615 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 616 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 617 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 618 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 619 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 620 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 621 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 622 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 623 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 624 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 625 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 626 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 627 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 628 total_reward 0.37735360253530725 trajectory_length 20\n",
      "Iteration 629 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 630 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 631 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 632 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 633 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 634 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 635 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 636 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 637 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 638 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 639 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 640 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 641 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 642 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 643 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 644 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 645 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 646 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 647 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 648 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 649 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 650 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 651 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 652 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 653 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 654 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 655 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 656 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 657 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 658 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 659 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 660 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 661 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 662 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 663 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 664 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 665 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 666 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 667 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 668 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 669 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 670 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 671 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 672 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 673 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 674 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 675 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 676 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 677 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 678 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 679 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 680 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 681 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 682 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 683 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 684 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 685 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 686 total_reward 0.3972143184582182 trajectory_length 19\n",
      "Iteration 687 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 688 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 689 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 690 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 691 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 692 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 693 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 694 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 695 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 696 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 697 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 698 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 699 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 700 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 701 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 702 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 703 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 704 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 705 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 706 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 707 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 708 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 709 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 710 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 711 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 712 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 713 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 714 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 715 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 716 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 717 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 718 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 719 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 720 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 721 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 722 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 723 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 724 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 725 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 726 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 727 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 728 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 729 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 730 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 731 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 732 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 733 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 734 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 735 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 736 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 737 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 738 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 739 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 740 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 741 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 742 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 743 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 744 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 745 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 746 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 747 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 748 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 749 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 750 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 751 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 752 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 753 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 754 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 755 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 756 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 757 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 758 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 759 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 760 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 761 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 762 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 763 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 764 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 765 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 766 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 767 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 768 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 769 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 770 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 771 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 772 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 773 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 774 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 775 total_reward 0.4181203352191771 trajectory_length 18\n",
      "Iteration 776 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 777 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 778 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 779 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 780 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 781 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 782 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 783 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 784 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 785 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 786 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 787 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 788 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 789 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 790 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 791 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 792 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 793 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 794 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 795 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 796 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 797 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 798 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 799 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 800 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 801 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 802 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 803 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 804 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 805 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 806 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 807 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 808 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 809 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 810 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 811 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 812 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 813 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 814 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 815 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 816 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 817 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 818 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 819 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 820 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 821 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 822 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 823 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 824 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 825 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 826 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 827 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 828 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 829 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 830 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 831 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 832 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 833 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 834 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 835 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 836 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 837 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 838 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 839 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 840 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 841 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 842 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 843 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 844 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 845 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 846 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 847 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 848 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 849 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 850 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 851 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 852 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 853 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 854 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 855 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 856 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 857 total_reward 0.46329123015975304 trajectory_length 16\n",
      "Iteration 858 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 859 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 860 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 861 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 862 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 863 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 864 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 865 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 866 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 867 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 868 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 869 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 870 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 871 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 872 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 873 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 874 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 875 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 876 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 877 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 878 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 879 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 880 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 881 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 882 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 883 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 884 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 885 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 886 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 887 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 888 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 889 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 890 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 891 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 892 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 893 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 894 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 895 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 896 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 897 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 898 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 899 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 900 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 901 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 902 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 903 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 904 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 905 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 906 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 907 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 908 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 909 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 910 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 911 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 912 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 913 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 914 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 915 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 916 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 917 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 918 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 919 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 920 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 921 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 922 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 923 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 924 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 925 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 926 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 927 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 928 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 929 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 930 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 931 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 932 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 933 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 934 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 935 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 936 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 937 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 938 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 939 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 940 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 941 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 942 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 943 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 944 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 945 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 946 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 947 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 948 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 949 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 950 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 951 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 952 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 953 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 954 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 955 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 956 total_reward 0.48767497911552954 trajectory_length 15\n",
      "Iteration 957 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 958 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 959 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 960 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 961 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 962 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 963 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 964 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 965 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 966 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 967 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 968 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 969 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 970 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 971 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 972 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 973 total_reward 0.44012666865176536 trajectory_length 17\n",
      "Iteration 974 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 975 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 976 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 977 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 978 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 979 total_reward 0.5133420832795048 trajectory_length 14\n",
      "Iteration 980 total_reward 0.5403600876626367 trajectory_length 13\n",
      "Iteration 981 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 982 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 983 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 984 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 985 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 986 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 987 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 988 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 989 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 990 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 991 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 992 total_reward 0.6302494097246091 trajectory_length 10\n",
      "Iteration 993 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 994 total_reward 0.5688000922764597 trajectory_length 12\n",
      "Iteration 995 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 996 total_reward 0.5987369392383787 trajectory_length 11\n",
      "Iteration 997 total_reward 0.6634204312890623 trajectory_length 9\n",
      "Iteration 998 total_reward 0.6983372960937497 trajectory_length 8\n",
      "Iteration 999 total_reward 0.44012666865176536 trajectory_length 17\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "discount_factor=0.95\n",
    "reward_history=[]\n",
    "trajectory_length_history=[]\n",
    "\n",
    "optimizer=keras.optimizers.Adam(0.02)\n",
    "for iteration in range(iterations):\n",
    "    state=start_state\n",
    "    total_reward=0\n",
    "    done=False\n",
    "    time_step=0\n",
    "    trajectory=[]\n",
    "    step_rewards=[]\n",
    "    action_choices=[]\n",
    "    while not done:\n",
    "        action = run_stochastic_policy(state)\n",
    "        trajectory.append(np.array(state, np.int32))\n",
    "        #print(\"time_step\",time_step,\"state\",state,\"action\",action)\n",
    "        next_state, reward, done = environment_step(action, state)\n",
    "        step_rewards.append(reward)\n",
    "        action_choices.append(action)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward*(discount_factor**time_step)\n",
    "        time_step+=1\n",
    "        if time_step>200:\n",
    "            # break trajectory.  This maze is currently too difficult for REINFORCE to complete a trajectory.\n",
    "            done=True\n",
    "            \n",
    "    trajectory_length=time_step\n",
    "    #  The next line is a bit of a hack which speeds up learning a lot.\n",
    "    #  This is a much smoother reward function curve.  It is a function which gets higher the smaller \n",
    "    # the trajectory length is. See e.g. https://www.wolframalpha.com/input/?i=plot+y%3D0.95%5Ex+from+x%3D0+to+x%3D200 for the graph\n",
    "    total_reward=(discount_factor**(trajectory_length-1)) # This works pretty well\n",
    "\n",
    "    trajectory=np.stack(trajectory,axis=0) # np.stack converts a list into a numpy array of shape [trajectory_shape,2]\n",
    "    action_choices=np.stack(action_choices,axis=0) # np.stack converts a list into a numpy array of shape [trajectory_shape]\n",
    "    baseline=0\n",
    "    grads=calculate_reinforce_gradient(trajectory, total_reward, baseline, action_choices)\n",
    "    optimizer.apply_gradients(zip([-grads], [table_action_probabilities_before_softmax])) # This updates the parameter vector.  Note the minus sign here (because tensorflow optimize functions assume you only ever want graident DESCENT, but we want ascent here) \n",
    "    \n",
    "    trajectory_length_history.append(trajectory_length)\n",
    "    reward_history.append(total_reward)\n",
    "    print(\"Iteration\",iteration,\"total_reward\",total_reward, \"trajectory_length\",trajectory_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphs\n",
    "- Should show performance improving over time....\n",
    "- This maze is theoretically solvable in 8 steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(trajectory_length_history)\n",
    "plt.ylabel('Trajectory Length (Solution Time)')\n",
    "plt.xlabel('Iteration')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit more theory (Optional section)\n",
    "\n",
    "When you see other people's implementations of REINFORCE, e.g. see Andrei Karpathy's [Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/), they often use the cross-entropy error function.  This is because it turns out that cross entropy loss function is very similar to what we need for REINFORCE (since they both have a log in them).\n",
    "\n",
    "Remember, the cross-entropy loss is defined to be:\n",
    "\\begin{align}\n",
    "L&=-\\sum_t log(P(y_t))\\\\\n",
    "\\end{align}\n",
    "where $P(y_t)$ means the probability that the neural network output gives to the true category $y_t$, and $t$ denotes the pattern index for each pattern in the training batch.\n",
    "\n",
    "With REINFORCE, in our code above, we did gradient ascent on L, where \\begin{align}\n",
    "L=(R-b) \\left[\\sum_t log(P(a_t))\\right]\\end{align}\n",
    "\n",
    "Hence the REINFORCE $L$ term looks almost the same as the Cross-Entropy loss.  They are the same apart from the REINFORCE experssion has an extra multiplication by $-(R-b)$.  As most neural-network training libraries come with gradient calculators for Cross-Entropy, we can use that code to do REINFORCE updates.  The trick is to replace the \"true data label\" $y_t$ for each pattern number $t$, which the cross-entropy function receives, with the action which your policy chose, i.e. you just need to replace $y_t$ by $a_t$, where $a_t$ is the actual action your policy chose at time-step $t$.  \n",
    "\n",
    "So we should be able to replace the code we wrote for $L$ by $L=-(R-b)\\times $cross_entropy(y_true=action_choices, y_pred=trajectory_action_probabilities), where cross_entropy=tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM).  If you want to try this in your submission to the auto-marker then do so.  Also, it means you can shorten the code quite a bit, and delete that awkward tf.gather line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experimentation ideas? (Optional section)\n",
    "\n",
    "- To get this method working on the larger maze (from the Q-learning example) we can use a discount factor = 0.98, and also use a baseline.  I found the baseline b=pow(0.98,200) worked, as this meant that trajectories which fail don't cause any learning to happen.  Only the trajectories that solve the maze cause learning to happen (and they teach it how to solve the maze correctly, which is what we want!)\n",
    "\n",
    "To add more sophisticated functionality:\n",
    "\n",
    "- We could add a GUI to show the 4 probabilities for each maze cell changing over time.\n",
    "- We could try to set the baseline intelligently.  You could set it as the average reward obtained from the previous 20 trajecties, and update your baseline using this recipee after every new trajectory finishes.\n",
    "- We could employ \"exploring starts\", to try to be able to solve large mazes.  Instead of starting from location (1,1) each trajetory, we start from a random valid location in the maze.\n",
    "- We could add mini-batching - group together N trajectories and find the average of their weight update, and apply that average all at once.  This should reduce the variance of the REINFORCE weight update, meaning we could use a higher learning rate.\n",
    "- We could try different reward functions, maybe one that uses a euclidean heuristic for all failed trajectories.\n",
    "- For the sake of improving your understanding, you could try doing the automatic differentiation by a hand calculation?\n",
    "- You could replace the table of probabilities by a keras-model (neural network)\n",
    "\n",
    "\n",
    "If you take any of this further, for your own interest, then please email me with what you come up with!  m.fairbank@essex.ac.uk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}